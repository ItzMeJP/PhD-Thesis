% Encoding: UTF-8

% Nossos

@article{ferreira2016stereo,
  title={Stereo-based real-time 6-DoF work tool tracking for robot programing by demonstration},
  author={Ferreira, Marcos and Costa, Paulo and Rocha, Lu{\'\i}s and Moreira, A. Paulo},
  journal={The International Journal of Advanced Manufacturing Technology},
  volume={85},
  number={1-4},
  pages={57--69},
  year={2016},
  publisher={Springer}
}

@article{de2020adaptpack,
  title={AdaptPack studio translator: translating offline programming to real palletizing robots},
  author={de Souza, Jo{\~a}o Pedro C. and Castro, Andr{\'e} L. and Rocha, Lu{\'\i}s F. and Silva, Manuel F.},
  journal={Industrial Robot: the international journal of robotics research and application},
  year={2020},
  publisher={Emerald Publishing Limited},
  DOI={https://doi.org/10.1108/IR-12-2019-0253}
}


@article{castro2020adaptpack,
  title={AdaptPack Studio: an automated intelligent framework for offline factory programming},
  author={Castro, Andr{\'e} L. and de Souza, Jo{\~a}o Pedro C. and Rocha, Lu{\'\i}s F. and Silva, Manuel F.},
  journal={Industrial Robot: the international journal of robotics research and application},
  year={2020},
  publisher={Emerald Publishing Limited},
  DOI={https://doi.org/10.1108/IR-12-2019-0252}
}

@inproceedings{souza2019converting,
  title={Converting robot offline programs to native code using the adaptpack studio translators},
  author={Souza, Jo{\~a}o Pedro and Castro, Andr{\'e} and Rocha, Lu{\'\i}s and Relvas, Pedro and Silva, Manuel F},
  booktitle={2019 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  pages={1--7},
  year={2019},
  organization={IEEE},
  DOI={10.1109/ICARSC.2019.8733631}
}

@inproceedings{castro2019adaptpack,
  title={Adaptpack studio: Automatic offline robot programming framework for factory environments},
  author={Castro, Andr{\'e} and \mkbibbold{Souza, Joao Pedro} and Rocha, Lu{\'\i}s and Silva, Manuel F},
  booktitle={2019 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  pages={1--6},
  year={2019},
  organization={IEEE},
  DOI={10.1109/ICARSC.2019.8733626}
}

@INPROCEEDINGS{9429788,

  author={de Souza, João Pedro C. and Rocha, Luís F. and Filipe, Vítor M. and Boaventura-Cunha, J. and Moreira, A. Paulo},

  booktitle={2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, 

  title={Low-Cost and Reduced-Size 3D-Cameras Metrological Evaluation Applied to Industrial Robotic Welding Operations}, 

  year={2021},

  volume={},

  number={},

  pages={123-129},

  doi={10.1109/ICARSC52212.2021.9429788}}




@article{bejczy1980sensors,
  title={Sensors, controls, and man-machine interface for advanced teleoperation},
  author={Bejczy, Antal K.},
  journal={Science},
  volume={208},
  number={4450},
  pages={1327--1335},
  year={1980},
  publisher={American Association for the Advancement of Science}
}

% Analytical

@article{salisbury1983kinematic,
  title={Kinematic and force analysis of articulated mechanical hands},
  author={Salisbury, J Kenneth and Roth, B},
  year={1983}
}

@article{diziouglu1984mechanics,
  title={Mechanics of form closure},
  author={Dizio{\u{g}}lu, B. and Lakshiminarayana, K.},
  journal={Acta mechanica},
  volume={52},
  number={1-2},
  pages={107--118},
  year={1984},
  publisher={Springer}
}

@inproceedings{Nguyen1987_1,
annote = {PArece ser um bom paper com otimas defini{\c{c}}{\~{o}}pes de equilibro e estabilidade, contudo nao {\'{e}}, ainda, o foco de meus estudos.},
author = {Nguyen, V.-D.},
booktitle = {Proceedings. 1987 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1987.1088008},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen - 1987 - Constructing Stable.pdf:pdf},
mendeley-groups = {Grasping/Analytical Graps Methods},
pages = {234--239},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Constructing stable grasps in 3D}},
url = {http://ieeexplore.ieee.org/document/1088008/},
volume = {4},
year = {1987}
}

@inproceedings{Nguyen1987_2,
abstract = {The author presents fast and simple algorithms for directly constructing force-closure grasps based on the shape of the grasped object. The synthesis of force-closure grasps constructs independent regions of contact for the fingertips, such that the motion of the grasped object is totally constrained. A force-closure grasp implies that equilibrium grasps exist. In the reverse direction, the author proves that nonmarginal equilibrium grasps with at least two soft-finger contacts, or three hard-finger contacts with friction, are also force-closure grasps.},
annote = {My opinion: This is a good paper with basic concepts

Defini{\c{c}}{\~{a}}o mais sutil e util para Form e Force closure},
author = {Nguyen, V.-D.},
booktitle = {Proceedings. 1987 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1987.1088014},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen - 1987 - Constructing Force-Closure.pdf:pdf},
isbn = {0818607874},
mendeley-groups = {Grasping/Analytical Graps Methods},
pages = {240--245},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{Constructing force-closure grasps in 3D}},
url = {http://ieeexplore.ieee.org/document/1088014/},
volume = {4},
year = {1987}
}

@article{Ponce1995,
abstract = {This paper addresses the problem of computing stable grasps of 2-D polygonal objects. We consider the case of a hand equipped with three hard fingers and assume point contact with friction. We prove new sufficient conditions for equilibrium and force closure that are linear in the unknown grasp parameters. This reduces computing the stable grasp regions in configuration space to constructing the three-dimensional projection of a five-dimensional polytope. We present an efficient projection algorithm based on linear programming and variable elimination among linear constraints. Maximal object segments where fingers can be positioned independently while ensuring force closure are found by linear optimization within the grasp regions. The approach has been implemented and several examples are presented},
annote = {Este paper possui uma grande lista com defini{\c{c}}{\~{o}}es e preposi{\c{c}}{\~{o}}es a cerca de equilibrio e grasp},
author = {Ponce, Jean and Faverjon, Bernard},
doi = {10.1109/70.478433},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponce, Faverjon - 1995 - On Computing Three-Finger Force-Closure Grasps of Polygonal Objects.pdf:pdf},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
mendeley-groups = {Grasping/Analytical Graps Methods},
number = {6},
pages = {868--881},
title = {{On Computing Three-Finger Force-Closure Grasps of Polygonal Objects}},
volume = {11},
year = {1995}
}

@article{Li2003,
abstract = {In this paper, we present a novel algorithm for computing three-finger force-closure grasps of two-dimensional (2-D) and three-dimensional (3-D) objects. In the case of a robot hand with three hard fingers and point contact with friction, new necessary and sufficient conditions for 2-D and 3-D equilibrium and force-closure grasps have been deduced, and a corresponding algorithm for computing force-closure grasps has been developed. Based on geometrical analysis, the algorithm is simple and only needs a few algebraic calculations. Finally, the algorithm has been implemented and its effectivity has been demonstrated by two examples.},
author = {{Jia-Wei Li} and {Hong Liu} and {He-Gao Cai}},
doi = {10.1109/TRA.2002.806774},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Liu, Cai - 2003 - On computing three-finger force-closure grasps of 2-D and 3-D objects.pdf:pdf},
issn = {1042-296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Equilibrium grasps,Force-closure grasps,Grasp planning,Multifingered robot hand},
mendeley-groups = {Grasping/Analytical Graps Methods},
month = {feb},
number = {1},
pages = {155--161},
title = {{On computing three-finger force-closure grasps of 2-d and 3-d objects}},
url = {http://ieeexplore.ieee.org/document/1177174/},
volume = {19},
year = {2003}
}

@inproceedings{Ferrari,
annote = {O paper e bom, base para muitos outros e no estudo do convex hull. O problema que ele aborda muito matematicamente alguns conceitos abstratos que nao fazer muito sentido numa aplica{\c{c}}{\~{a}}o pratica.},
author = {Ferrari, C. and Canny, J.},
booktitle = {Proceedings 1992 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1992.219918},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Planning Optimal Grasps.pdf:pdf},
isbn = {0-8186-2720-4},
mendeley-groups = {Grasping/Analytical Graps Methods},
pages = {2290--2295},
year = {1992},
publisher = {IEEE Comput. Soc. Press},
title = {{Planning optimal grasps}},
url = {http://ieeexplore.ieee.org/document/219918/}
}

@article{bicchi1995closure,
  title={On the closure properties of robotic grasping},
  author={Bicchi, Antonio},
  journal={The International Journal of Robotics Research},
  volume={14},
  number={4},
  pages={319--334},
  year={1995},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@inproceedings{Bicchi2000,
annote = {Boas defini{\c{c}}oes e referencias de modelagem de grasp, mas o paper ta um pouco antigo},
author = {Bicchi, Antonio and Kumar, Vijay},
booktitle = {Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
doi = {10.1109/ROBOT.2000.844081},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bicchi, Kumar - 2000 - 19. Robotic Grasping and Contact.pdf:pdf},
isbn = {0-7803-5886-4},
mendeley-groups = {Bin Picking/Survey},
pages = {348--353},
publisher = {IEEE},
title = {{Robotic grasping and contact: a review}},
url = {http://ieeexplore.ieee.org/document/844081/},
volume = {1},
year = {2000}
}

@article{Roa2014,
abstract = {{\textcopyright} 2014, The Author(s). The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
author = {Roa, M{\'{a}}ximo A. and Su{\'{a}}rez, Ra{\'{u}}l},
doi = {10.1007/s10514-014-9402-3},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roa, Su{\'{a}}rez - 2014 - Grasp quality measures review and performance.pdf:pdf},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {Grasp quality,Grasping,Manipulation,Robotic hands},
mendeley-groups = {Bin Picking/Survey,Grasping/Survey},
month = {jan},
number = {1},
pages = {65--88},
title = {{Grasp quality measures: review and performance}},
url = {http://link.springer.com/10.1007/s10514-014-9402-3},
volume = {38},
year = {2015}
}

@article{liu2004complete,
  title={A complete and efficient algorithm for searching 3-D form-closure grasps in the discrete domain},
  author={Liu, Yun-Hui and Lam, Miu-Ling and Ding, Dan},
  journal={IEEE Transactions on Robotics},
  volume={20},
  number={5},
  pages={805--816},
  year={2004},
  publisher={IEEE}
}

@inproceedings{el2009computing,
  title={On computing robust N-finger force-closure grasps of 3D objects},
  author={El-Khoury, Sahar and Sahbani, Anis},
  booktitle={2009 IEEE international conference on robotics and automation},
  pages={2480--2486},
  year={2009},
  organization={IEEE}
}

%Livro resumindo boa parte da teoria de Grasping.
@incollection{prattichizzo2016grasping,
  title={Grasping},
  author={Prattichizzo, Domenico and Trinkle, Jeffrey C},
  booktitle={Springer handbook of robotics},
  pages={955--988},
  year={2016},
  publisher={Springer}
}

@article{ponce1995computing,
  title={On computing three-finger force-closure grasps of polygonal objects},
  annote = {Antiga REF (7) do deliverable},
  author={Ponce, Jean and Faverjon, Bernard},
  journal={IEEE Transactions on robotics and automation},
  volume={11},
  number={6},
  pages={868--881},
  year={1995},
  publisher={IEEE}
}

@article{li2003computing,
  title={On computing three-finger force-closure grasps of 2-D and 3-D objects},
  author={Li, Jia-Wei and Liu, Hong and Cai, He-Gao},
  annote = {Antiga REF (8) do deliverable},
  journal={IEEE Transactions on Robotics and Automation},
  volume={19},
  number={1},
  pages={155--161},
  year={2003},
  publisher={IEEE}
}

@article{liu1999qualitative,
  title={Qualitative test and force optimization of {3-D} frictional form-closure grasps using linear programming},
  annote = {Antiga REF (10) do deliverable},
  author={Liu, Yun-Hui},
  journal={IEEE Transactions on Robotics and Automation},
  volume={15},
  number={1},
  pages={163--173},
  year={1999},
  publisher={IEEE}
}

@article{liu2000computing,
  title={Computing n-finger form-closure grasps on polygonal objects},
  annote = {Antiga REF (11) do deliverable},
  author={Liu, Yun-Hui},
  journal={The International journal of robotics research},
  volume={19},
  number={2},
  pages={149--158},
  year={2000},
  publisher={SAGE Publications}
}

@inproceedings{ding2000computing,
  title={Computing 3-D optimal form-closure grasps},
  annote = {Antiga REF (12) do deliverable},
  author={Ding, Dan and Liu, Yun-Hui and Wang, Shuguo},
  booktitle={Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)},
  volume={4},
  pages={3573--3578},
  year={2000},
  organization={IEEE}
}

% Analitical + Primitive
@inproceedings{Jain2016,
abstract = {In this paper, we present a novel grasp detection algorithm targeted towards assistive robotic manipulation systems. We consider the problem of detecting robotic grasps using only the raw point cloud depth data of a scene containing unknown objects, and apply a geometric approach that categorizes objects into geometric shape primitives based on an analysis of local surface properties. Grasps are detected without a priori models, and the approach can generalize to any number of novel objects that fall within the shape primitive categories. Our approach generates multiple candidate object grasps, which moreover are semantically meaningful and similar to what a human would generate when teleoperating the robot-and thus should be suitable manipulation goals for assistive robotic systems. An evaluation of our algorithm on 30 household objects includes a pilot user study, confirms the robustness of the detected grasps and was conducted in real-world experiments using an assistive robotic arm.},
author = {Jain, Siddarth and Argall, Brenna},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487348},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/Grasp Detection for Assistive Robotic Manipulation.pdf:pdf},
isbn = {978-1-4673-8026-3},
issn = {10504729},
mendeley-groups = {Grasping/Analytical Graps Methods,To READ},
month = {may},
pages = {2015--2021},
publisher = {IEEE},
title = {{Grasp detection for assistive robotic manipulation}},
url = {http://ieeexplore.ieee.org/document/7487348/},
volume = {2016-June},
year = {2016}
}

@inproceedings{Miller2003,
abstract = {Automatic grasp planning for robotic hands is a difficult problem because of the huge number of possible hand configurations. However, humans simplify the problem by choosing an appropriate prehensile posture appropriate for the object and task to be performed. By modeling an object as a set of shape primitives, such as spheres, cylinders, cones and boxes, we can use a set of NI- to generate a set of grasp starting positions and pregrasp shapes that can then be tested on the object model. Each grasp is tested and evaluated within our grasping simulator “GraspIt!”, and the best grasps are presented to the user. The simulator can also plan grasps in a complex environment involving obstacles and the reachability constraints of a robot arm.},
annote = {Os autores definem algumas formas b{\'{a}}sicas e associam a elas os grasp points atrelados ao tipo de gripper. Depois os outros objetos ser{\~{a}}o uma associa{\c{c}}{\~{a}}o dessas formas basicas.

Logo se eu quero fazer para varios tipos de gripper, teria que criar o banco de dados de todos gripper atrelados aos simple shapes.

Este paper {\'{e}} data-driven, porem nao {\'{e}} baseado em experiencia... Como devo formular?},
author = {Miller, AT and Knoop, S and Christensen, HI and Allen, PK},
booktitle = {2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422)},
doi = {10.1109/ROBOT.2003.1241860},
isbn = {0-7803-7736-2},
mendeley-groups = {Grasping/Data-driven/know object/based on model,Grasping/Analytical Graps Methods},
pages = {1824--1829},
publisher = {IEEE},
title = {{Automatic grasp planning using shape primitives}},
url = {http://ieeexplore.ieee.org/document/1241860/},
year = {2003}
}

@inproceedings{Goldfeder2007,
abstract = {Planning realizable and stable grasps on 3D objects is crucial for many robotics applications, but grasp planners often ignore the relative sizes of the robotic hand and the object being grasped or do not account for physical joint and positioning limitations. We present a grasp planner that can consider the full range of parameters of a real hand and an arbitrary object, including physical and material properties as well as environmental obstacles and forces, and produce an output grasp that can be immediately executed. We do this by decomposing a 3D model into a superquadric 'decomposition tree' which we use to prune the intractably large space of possible grasps into a subspace that is likely to contain many good grasps. This subspace can be sampled and evaluated in GraspIt!, our 3D grasping simulator, to find a set of highly stable grasps, all of which are physically realizable. We show grasp results on various models using a Barrett hand.},
annote = {KW:
Analytical Approach (indirectly with GraspIT). Data-driven. 3D model based 

The authors of XXX proposes a grasp planning by decomposition trees based on superquadratic representation of the 3D model of the object to be grasped. For each superquadratic surface it is sampled some grasp points and them it is evaluated using the GraspIt to get the grasp quality. This quality is based on : “..finding grasps that are not merely force closed, but that are closed to reasonably large forces and torques”.},
author = {Goldfeder, Corey and Allen, Peter K. and Lackner, Claire and Pelossof, Raphael},
booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.364200},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldfeder et al. - 2007 - Grasp planning via decomposition trees.pdf:pdf},
isbn = {1-4244-0602-1},
issn = {1050-4729},
mendeley-groups = {Grasping/Data-driven/know object/based on model},
month = {apr},
number = {April},
pages = {4679--4684},
publisher = {IEEE},
title = {{Grasp Planning via Decomposition Trees}},
url = {http://ieeexplore.ieee.org/document/4209818/},
year = {2007}
}

@article{Aleotti2012,
abstract = {Neuro-psychological findings have shown that human perception of objects is based on part decomposition. Most objects are made of multiple parts which are likely to be the entities actually involved in grasp affordances. Therefore, automatic object recognition and robot grasping should take advantage from 3D shape segmentation. This paper presents an approach toward planning robot grasps across similar objects by part correspondence. The novelty of the method lies in the topological decomposition of objects that enables high-level semantic grasp planning. In particular, given a 3D model of an object, the representation is initially segmented by computing its Reeb graph. Then, automatic object recognition and part annotation are performed by applying a shape retrieval algorithm. After the recognition phase, queries are accepted for planning grasps on individual parts of the object. Finally, a robot grasp planner is invoked for finding stable grasps on the selected part of the object. Grasps are evaluated according to a widely used quality measure. Experiments performed in a simulated environment on a reasonably large dataset show the potential of topological segmentation to highlight candidate parts suitable for grasping. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Aleotti, Jacopo and Caselli, Stefano},
doi = {10.1016/j.robot.2011.07.022},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/aleotti2012.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Grasping,Object recognition,Shape segmentation},
mendeley-groups = {Grasping/Analytical Graps Methods},
number = {3},
pages = {358--366},
publisher = {Elsevier B.V.},
title = {{A 3D shape segmentation approach for robot grasping by parts}},
url = {http://dx.doi.org/10.1016/j.robot.2011.07.022},
volume = {60},
year = {2012}
}

@article{Przybylski2011,
abstract = {Many supporting activities that future service robots might perform in people's homes depend on the capability to grasp and manipulate arbitrary objects. Easily accomplished by humans, but very difficult to achieve for robots, grasping involves dealing with a high-dimensional space of parameters which include hand kinematics, object geometry, material properties and forces. We believe that the way a robot grasps an object should be motivated by the object's geometry and that the search space for stable grasps can be dramatically reduced if the underlying object representation reflects symmetry properties of the object that contain valuable information for grasp planning. In this paper, we introduce the grid of medial spheres, a volumetric object representation based on the medial axis transform. The grid of medial spheres represents arbitrarily shaped objects with arbitrary levels of detail and contains symmetry information that can be easily exploited by a grasp planning algorithm. We present the data structure as well as a grasp planning algorithm that exploits it and provide experimental results on various object models using two robot hands in simulation.},
annote = {{\'{E}} data driven, contudo o grasp quaility {\'{e}} feito com a m{\'{e}}trica analitica do Ferrari.

A ideia base do paper {\'{e}} boa, nao sei quanto a restri{\c{c}}{\~{a}}o de simetria e como devo abordar a parte do angulo do objeto escolhido...},
author = {Przybylski, Markus and Asfour, Tamim and Dillmann, R{\"{u}}diger},
doi = {10.1109/IROS.2011.6048625},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Przybylski, Asfour, Dillmann - 2011 - Planning grasps for robotic hands using a novel object representation based on the medial axis tra.pdf:pdf},
isbn = {9781612844541},
journal = {IEEE International Conference on Intelligent Robots and Systems},
mendeley-groups = {Grasping/Data-driven/know object/based on model},
pages = {1781--1788},
title = {{Planning grasps for robotic hands using a novel object representation based on the medial axis transform}},
year = {2011}
}





% Optimization
@phdthesis{AndrewT2004,
annote = {Thesis que originou o graspit!},
author = {{Andrew T}, Miller and {Peter K}, Allen},
file = {:home/joaopedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrew T, Peter K - 2004 - GraspIt! A Versatile Simulator for Robotic Grasping.pdf:pdf},
keywords = {graspit{\_}th},
mendeley-tags = {graspit{\_}th},
pages = {140},
school = {Columbia University},
title = {{GraspIt! A Versatile Simulator for Robotic Grasping}},
url = {http://www.cs.columbia.edu/{~}cmatei/graspit/pdf/GraspIt{\_}RA04-complete-system.pdf{\%}5Cnpapers2://publication/uuid/1054081B-B727-46D2-B0FA-9364E77F53CF},
year = {2001}
}

@article{rakesh2018optimizing,
  title={Optimizing force closure grasps on 3D objects using a modified genetic algorithm},
  author={Rakesh, Venkataramani and Sharma, Utkarsh and Murugan, S and Venugopal, S and Asokan, T},
  journal={Soft Computing},
  volume={22},
  number={3},
  pages={759--772},
  year={2018},
  publisher={Springer}
}





% >Learning

@article{Oztop2001,
abstract = {We propose and implement a learning to grasp system inspired from the development of reaching and grasping in infants, and the neurophysiology of the monkey premotor cortex. The system is composed of a virtual 19 DOF kinematics arm/hand and a learning mechanism that enables it to perform a successful grasp. The learning is based on "motor babbling". The model performs open hand reaches to the vicinity of the targets, which human infants younger than 4 moths of age appear to do. The contact of the hand with the object triggers an enclosure of the hand simulating the palmer reflex, characteristic to infants that are younger than 6 months of age. The varying degree of enclosure of each finger and the randomness in the reaching phase enables the system to explore the grasp configuration space. The learning scheme employed is a Hebbian one.},
author = {Oztop, E. and Arbib, M. A.},
doi = {10.1109/iembs.2001.1019077},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/a410502.pdf:pdf},
issn = {04549244},
journal = {Annual Reports of the Research Reactor Institute, Kyoto University},
keywords = {Grasping,Hebbian learning,Infant,Reaching},
mendeley-groups = {Grasping/Data-driven/Learning},
pages = {857--860},
title = {{A biologically inspired learning to grasp system}},
volume = {1},
year = {2001}
}

@article{Pelossof2004,
abstract = {Finding appropriate stable grasps for a hand (either robotic or human) on an arbitrary object has proved to be a challenging and difficult problem. The space of grasping parameters coupled with the degrees-of-freedom and geometry of the object to be grasped creates a high-dimensional, non- smooth manifold. Traditional search methods applied to this manifold are typically not powerful enough to find appropriate stable grasping solutions, let alone optimal grasps. We address this issue in this paper, which attempts to find optimal grasps of objects using a grasping simulator. Our unique approach to the problem involves a combination of numerical methods to recover parts of the grasp quality surface with any robotic hand, and contemporary machine learning methods to interpolate that surface, in order to find the optimal grasp.},
author = {Pelossof, Raphael and Miller, Andrew and Allen, Peter and Jebara, Tony},
doi = {10.1109/robot.2004.1308797},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/svmgraspra04.pdf:pdf},
isbn = {0780382323},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
mendeley-groups = {Grasping/Data-driven/Learning},
number = {4},
pages = {3512--3518},
title = {{An SVM learning approach to robotic grasping}},
volume = {2004},
year = {2004}
}

@article{dini2000planning,
  title={Planning grasps for industrial robotized applications using neural networks},
  author={Dini, Gino and Failli, Franco},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={16},
  number={6},
  pages={451--463},
  year={2000},
  publisher={Elsevier}
}

@incollection{tenPas,
annote = {This paper proposes a grasp estimator for two-finger grasp without previously known object shape. They use an analytical method (based on antipodal assumptions and geometric shape of point clouds) without machine learning and achieved a success rate of 73{\%} in isolated objects. Using SVM this rate raised to 87.8{\%}. In the case of bin picking the result with SVM went to 73{\%}},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {ten Pas, Andreas and Platt, Robert},
doi = {10.1007/978-3-319-51532-8_19},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/tenpas2017.pdf:pdf},
isbn = {9783319515328},
issn = {1610-7438},
mendeley-groups = {Grasping,Grasping/Data-driven},
pages = {307--324},
pmid = {15003161},
title = {{Using Geometry to Detect Grasp Poses in 3D Point Clouds}},
url = {http://link.springer.com/10.1007/978-3-319-51532-8{\_}19},
year = {2018}
}

@inproceedings{seita2016large,
  title={Large-scale supervised learning of the grasp robustness of surface patch pairs},
  author={Seita, Daniel and Pokorny, Florian T and Mahler, Jeffrey and Kragic, Danica and Franklin, Michael and Canny, John and Goldberg, Ken},
  booktitle={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)},
  pages={216--223},
  year={2016},
  organization={IEEE}
}
@article{Trottier2017,
abstract = {The ability to grasp ordinary and potentially never-seen objects is an important task in both domestic and industrial robotics. For a system to accomplish this, it must autonomously identify grasping locations by using information from various sensors, such as Microsoft Kinect 3D camera. Despite numerous progress, significant work still remains to be done for this task. To this effect, we propose a dictionary learning and sparse representation (DLSR) framework for representing RGBD images from 3D sensors in the context of identifying grasping locations. In contrast to previously proposed approaches that relied on sophisticated regularization or very large datasets, our derived perception system has a fast training phase and can work with small datasets. It is also theoretically founded for dealing with masked-out entries, which are common with 3D sensors. We contribute by presenting a comparative study of several DLSR approach combinations for recognizing and detecting grasp candidates on the standard Cornell dataset. Experimental results show a performance improvement of 1.69{\%} in detection and 3.16{\%} in recognition over current state-of-The-Art convolutional neural network (CNN). Even though nowadays most popular vision-based approach is CNN, this suggests that DLSR is also a viable alternative with interesting advantages that CNN has not.},
annote = {I like this paper, it is a good paper, since it use a learning polices more fast to trainning and not require a huge amount of database like CNN. Of course, it anchieved good results... 

The problem is, they use the rectangle representation and Jaccard metric... 

The biggest problem, the detection time is very high. 

A nice future work proposal realted is: use DSLR+CNN. One speed the trainning phase and the otherthe detection phase.},
author = {Trottier, Ludovic and Giguere, Philippe and Chaib-Draa, Brahim},
doi = {10.1109/WACV.2017.102},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trottier, Giguere, Chaib-Draa - 2017 - Sparse dictionary learning for identifying grasp locations.pdf:pdf},
isbn = {9781509048229},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
keywords = {dlsr2017},
mendeley-groups = {Grasping/Data-driven/Learning},
mendeley-tags = {dlsr2017},
pages = {871--879},
title = {{Sparse dictionary learning for identifying grasp locations}},
year = {2017}
}


%% >Reinforced Learning

@article{Rezzoug2002,
abstract = {In this work, we focus our interest on hand grasping posture definition from few knowledge. For that a multistage neural network architecture is proposed that implements a reinforcement learning scheme on real valued outputs. Simulations results show good learning of grasping postures of various types of objects, with different numbers of fingers involved and different contacts configurations.},
author = {Rezzoug, Nasser and Gorce, Philippe},
doi = {10.1109/irds.2002.1044001},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/a-multistage-neural-network-architecture-to-learn-hand-grasping-.pdf:pdf},
isbn = {0780373987},
journal = {IEEE International Conference on Intelligent Robots and Systems},
mendeley-groups = {Grasping/Data-driven/Learning},
number = {October},
pages = {1705--1710},
title = {{A multistage neural network architecture to learn hand grasping posture}},
volume = {2},
year = {2002}
}

@article{Wheeler2002,
abstract = {When interacting with an object, the possible choices of grasping and manipulation operations are often limited by pick-and-place constraints. Traditional planning methods are analytical in nature and require geometric models of parts, fixtures and motions to identify and avoid the constraints. These methods can easily become computationally expensive and are often brittle under model or sensory uncertainty. In contrast, infants do not construct complete models of the objects that they manipulate, but instead appear to incrementally construct models based on interaction with the objects themselves. We propose that robotic pick-and-place operations can be formulated as prospective behavior and that an intelligent agent can use interaction with the environment to learn strategies which accommodate the constraints based on expected future success. We present experiments demonstrating this technique, and compare the strategies utilized by the agent to the behaviors observed in young children when presented with a similar task.},
author = {Wheeler, D. S. and Fagg, A. H. and Grupen, R. A.},
doi = {10.1109/DEVLRN.2002.1011865},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/download.pdf:pdf},
isbn = {0769514596},
journal = {Proceedings - 2nd International Conference on Development and Learning, ICDL 2002},
mendeley-groups = {Grasping/Data-driven/Learning},
pages = {197--202},
title = {{Learning prospective pick and place behavior}},
year = {2002}
}



@article{Saxena2008,
abstract = {We consider the problem of grasping novel objects, specifically objects that are being seen for the first time through vision. Grasping a previously unknown object, one for which a 3-d model is not available, is a challenging problem. Furthermore, even if given a model, one still has to decide where to grasp the object. We present a learning algorithm that neither requires nor tries to build a 3-d model of the object. Given two (or more) images of an object, our algorithm attempts to identify a few points in each image corresponding to good locations at which to grasp the object. This sparse set of points is then triangulated to obtain a 3-d location at which to attempt a grasp. This is in contrast to standard dense stereo, which tries to triangulate every single point in an image (and often fails to return a good 3-d model). Our algorithm for identifying grasp locations from an image is trained by means of supervised learning, using synthetic images for the training set. We demonstrate this approach on two robotic manipulation platforms. Our algorithm successfully grasps a wide variety of objects, such as plates, tape rolls, jugs, cellphones, keys, screwdrivers, staplers, a thick coil of wire, a strangely shaped power horn and others, none of which were seen in the training set. We also apply our method to the task of unloading items from dishwashers.},
author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y},
doi = {10.1177/0278364907087172},
file = {:C$\backslash$:/Users/Avell B155 MAX/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxena, Driemeyer, Ng - 2018 - Robotic Grasping of Novel Objects.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
mendeley-groups = {Grasping,Bin Picking/Pose Estimation},
month = {feb},
number = {2},
pages = {157--173},
title = {{Robotic Grasping of Novel Objects using Vision}},
url = {http://journals.sagepub.com/doi/10.1177/0278364907087172},
volume = {27},
year = {2008}
}

@inproceedings{Jiang2011a,
abstract = {Given an image and an aligned depth map of an object, our goal is to estimate the full 7-dimensional gripper configuration-its 3D location, 3D orientation and the gripper opening width. Recently, learning algorithms have been successfully applied to grasp novel objects-ones not seen by the robot before. While these approaches use low-dimensional representations such as a 'grasping point' or a 'pair of points' that are perhaps easier to learn, they only partly represent the gripper configuration and hence are sub-optimal. We propose to learn a new 'grasping rectangle' representation: an oriented rectangle in the image plane. It takes into account the location, the orientation as well as the gripper opening width. However, inference with such a representation is computationally expensive. In this work, we present a two step process in which the first step prunes the search space efficiently using certain features that are fast to compute. For the remaining few cases, the second step uses advanced features to accurately select a good grasp. In our extensive experiments, we show that our robot successfully uses our algorithm to pick up a variety of novel objects. {\textcopyright} 2011 IEEE.},
annote = {Este paper tem uma revisao bib interessante quanto a algoritmos de generaliza{\c{c}}{\~{a}}o analiticas...},
author = {{Yun Jiang} and Moseson, Stephen and Saxena, Ashutosh},
booktitle = {2011 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980145},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/yunjiang2011.pdf:pdf},
isbn = {978-1-61284-386-5},
issn = {10504729},
mendeley-groups = {Grasping/Data-driven},
month = {may},
pages = {3304--3311},
publisher = {IEEE},
title = {{Efficient grasping from RGBD images: Learning using a new rectangle representation}},
url = {http://ieeexplore.ieee.org/document/5980145/},
year = {2011}
}

@inproceedings{Zeng2018,
abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu .},
annote = {Deep RL},
archivePrefix = {arXiv},
arxivId = {1803.09956},
author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2018.8593986},
eprint = {1803.09956},
file = {:home/joaopedro/Downloads/1803.09956.pdf:pdf},
isbn = {978-1-5386-8094-0},
issn = {21530866},
mendeley-groups = {To READ/Reinforcement Learning},
month = {oct},
pages = {4238--4245},
publisher = {IEEE},
title = {{Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning}},
url = {https://ieeexplore.ieee.org/document/8593986/},
year = {2018}
}

@inproceedings{boularias2015learning,
  title={Learning to manipulate unknown objects in clutter by reinforcement},
  author={Boularias, Abdeslam and Bagnell, James Andrew and Stentz, Anthony},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{platt2007learning,
  title={Learning grasp strategies composed of contact relative motions},
  author={Platt, Robert},
  booktitle={2007 7th IEEE-RAS International Conference on Humanoid Robots},
  pages={49--56},
  year={2007},
  organization={IEEE}
}

@inproceedings{BaierLowenstein2007,
abstract = {Although grasping of everyday objects has been a research topic over the last decades, it still is a crucial task for service robots. Several methods have been proposed to generate suitable grasps for objects. Many of them are restricted to a certain type of grasp or limited to a fixed number of contacts. In this paper we propose an algorithm based on reinforcement learning, to enable a service robot to grasp every kind of object with as many contacts as needed. The proposed method will be evaluated using a simulation with a three-fingered robotic hand. {\textcopyright}2007 IEEE.},
author = {Baier-Lowenstein, Tim and {Jianwei Zhang}},
booktitle = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2007.4399053},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baier-L{\"{o}}wenstein, Zhang - 2007 - Learning to grasp everyday objects using reinforcement-learning with automatic value cut-off.pdf:pdf},
isbn = {978-1-4244-0911-2},
mendeley-groups = {To READ/Reinforcement Learning},
month = {oct},
pages = {1551--1556},
publisher = {IEEE},
title = {{Learning to grasp everyday objects using reinforcement-learning with automatic value cut-off}},
url = {http://ieeexplore.ieee.org/document/4399053/},
year = {2007}
}

@inproceedings{Bernd2002,
author = {Rossler, B. and {Jianwei Zhang} and Knoll, A.},
booktitle = {Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292)},
doi = {10.1109/ROBOT.2002.1014336},
file = {:C$\backslash$:/Users/JoaoPedro/Downloads/1290557.pdf:pdf},
isbn = {0-7803-7272-7},
mendeley-groups = {To READ/Reinforcement Learning},
pages = {3912--3917},
publisher = {IEEE},
title = {{Visual guided grasping of aggregates using self-valuing learning}},
url = {http://ieeexplore.ieee.org/document/1014336/},
volume = {4}
}


% >Deep Learning

@article{morrison2020learning,
  title={Learning robust, real-time, reactive robotic grasping},
  author={Morrison, Douglas and Corke, Peter and Leitner, J{\"u}rgen},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={2-3},
  pages={183--201},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{choi2018learning,
  title={Learning object grasping for soft robot hands},
  author={Choi, Changhyun and Schwarting, Wilko and DelPreto, Joseph and Rus, Daniela},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={3},
  pages={2370--2377},
  year={2018},
  publisher={IEEE}
}


@article{Zeng2019,
author = {Zeng, Andy and Song, Shuran and Yu, Kuan-Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and {Chavan Dafle}, Nikhil and Holladay, Rachel and Morona, Isabella and Nair, Prem Qu and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
doi = {10.1177/0278364919868017},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Amazon Robotics Challenge,active perception,affordance learning,cross-domain image matching,deep learning,grasping,one-shot recognition,pick-and-place,vision for manipulation},
month = {aug},
pages = {027836491986801},
title = {Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching},
url = {http://journals.sagepub.com/doi/10.1177/0278364919868017},
year = {2019}
}


@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
annote = {Interessante e sutil revisao bibliografica com papers atuais. 

Posso olhar os trabalhos que ele relacionou que utilizaram metricas de learning e generaliza{\c{c}}{\~{a}}o sem o DeepLearning...

A two-step cascaded system with two deep networks is proposed. The first detects grasp candidate approximately accurate. The second one, with more features used, give more accurate detections.

Eu pulei como foi feita a metologia visto que existe papers com melhores resultados. Mas convem eu ler, caso eu queia aplicar DP

Ele aborda as definicao da metrica de ponto e de retangulo.},
archivePrefix = {arXiv},
arxivId = {1301.3592},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
doi = {10.1177/0278364914549607},
eprint = {1301.3592},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/lenz2015.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3D feature learning,Baxter,PR2,RGB-D multi-modal data,Robotic grasping,deep learning,lenzDP},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
mendeley-tags = {lenzDP},
month = {apr},
number = {4-5},
pages = {705--724},
title = {{Deep learning for detecting robotic grasps}},
url = {http://journals.sagepub.com/doi/10.1177/0278364914549607},
volume = {34},
year = {2015}
}

@inproceedings{Redmon2015,
abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
annote = {Understanding bullets: 

The authors aplly the AlexNet CNN to solve the detect grasp in three manners (more precise two manners): direct regression; multigrasp and regression+classification; 

DirectRegression: use a previously trainned AlexNet (transfered learning from the ImageNet dataset) and use a direct RGD raw image. The last FC NN do the classification of ONE grasp candidate; 

MutiGrasp: is the same as direct approach, however a the RGD image is meshed in NxN small images. Each block elect a candidate. A mask define the probability and chosen the best one; 

Clasification + regression: only show taht is possible to classificate the object together. It is obviously, since they use an already trainned CNN in ImageNet classification setup. 

Opinion/Question Bullets 

The authors got a better perfomance than “Deep learning for detecting robotic grasps in grasp calssification” (its base paper)with a sucess rate of 87.1{\%}) However, the methodology is only focus in grasp classification and not perfoms the grasping itself. It does not confirm that the prediction of rectangle grasp {\"{i}}s enough as related by ‘'Review of Deep Learning Methods in Robotic Grasp Detection“. 

The speedup is because they use GPU and does not perfom a window sliding over the image. They use the image directly. 

They remove the blue channel to be used by AlexNet, maybe aggregate it in two networks could increase the performance (like the proposal of “Robotic Grasp Detection using Deep Convolutional Neural Network”). 

Same problems related: rectangle grasping approach; rectangle evaluation metric; do not perfom grasp to see the real problems},
archivePrefix = {arXiv},
arxivId = {1412.3128},
author = {Redmon, Joseph and Angelova, Anelia},
booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139361},
eprint = {1412.3128},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/redmon2015.pdf:pdf},
isbn = {978-1-4799-6923-4},
issn = {10504729},
keywords = {redmon2015},
mendeley-groups = {Grasping/Data-driven},
mendeley-tags = {redmon2015},
month = {may},
number = {June},
pages = {1316--1322},
publisher = {IEEE},
title = {{Real-time grasp detection using convolutional neural networks}},
url = {http://ieeexplore.ieee.org/document/7139361/},
volume = {2015-June},
year = {2015}
}

@article{Kumra2017,
abstract = {Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.2analitical1{\%} on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.},
annote = {Deep CNN is a proposal of this paper. For this, they proposed the use of the newest methods, the ResNet. With a deeper network, in comparison with previosly works, they anchieve a grasp estimation sucess rate of 89.21{\%}. It is more fast and applicable too. However, they do not perform the graps and the evaluation is only compared with a groundtruth database. Another significant issue is that it is only applied to 2 figered (paralel) gripper. One issue of Deep Archtectures for grasp is the GPU. Therefore, the robot need to be equiped with it},
archivePrefix = {arXiv},
arxivId = {1611.08036},
author = {Kumra, Sulabh and Kanan, Christopher},
doi = {10.1109/IROS.2017.8202237},
eprint = {1611.08036},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/kumra2017.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Deep Learning in Robotics and Automation,Grasping,},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
pages = {769--776},
title = {{Robotic grasp detection using deep convolutional neural networks}},
volume = {2017-Septe},
year = {2017}
}

@incollection{Watson2017,
abstract = {Evaluating the Capabilities of a Flight-Style Swarm AUV to Perform Emergent and Adaptive BehavioursThrough simulation, this paper evaluates the capabilities of the EcoSUB{\$}{\$}$\backslash$mu {\$}{\$}$\mu$, a small, low cost Autonomous Underwater Vehicle (AUV), to perform emergent and adaptive behaviours for environmental m... {\%}$\backslash$ 2018-06-05 17:13:00},
annote = {This paper bassically reprodused Redmon proposal in real robot with few differences. Unsuscefully, they cannot replicate the detection sucess rate, however they try to apply it in the real scenario showing the difficults and 78{\%} sucess rate},
author = {Watson, Joe and Hughes, Josie and Iida, Fumiya},
booktitle = {Conference Towards Autonomous Robotic Systems},
doi = {10.1007/978-3-319-64107-2_50},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/watson2017.pdf:pdf},
isbn = {9783319641072},
keywords = {3d printing,manipulation,soft robotics,watson2017real},
pages = {617--626},
title = {{Real-World, Real-Time Robotic Grasping with Convolutional Neural Networks}},
url = {http://link.springer.com/10.1007/978-3-319-64107-2{\_}50},
volume = {2},
year = {2017}
}

@inproceedings{Guo2017,
abstract = {The robotic grasp detection is a great challenge in the area of robotics. Previous work mainly employs the visual approaches to solve this problem. In this paper, a hybrid deep architecture combining the visual and tactile sensing for robotic grasp detection is proposed. We have demonstrated that the visual sensing and tactile sensing are complementary to each other and important for the robotic grasping. A new THU grasp dataset has also been collected which contains the visual, tactile and grasp configuration information. The experiments conducted on a public grasp dataset and our collected dataset show that the performance of the proposed model is superior to state of the art methods. The results also indicate that the tactile data could help to enable the network to learn better visual features for the robotic grasp detection task.},
author = {Guo, Di and Sun, Fuchun and Liu, Huaping and Kong, Tao and Fang, Bin and Xi, Ning},
booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989191},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/guo2017.pdf:pdf},
isbn = {978-1-5090-4633-1},
issn = {10504729},
keywords = {Computer Vision for Other Robotic Applica,Grasping},
mendeley-groups = {Grasping/Data-driven},
month = {may},
pages = {1609--1614},
publisher = {IEEE},
title = {{A hybrid deep architecture for robotic grasp detection}},
url = {http://ieeexplore.ieee.org/document/7989191/},
year = {2017}
}

@article{TenPas2017,
abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75{\%} and 95{\%} for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93{\%} end-to-end grasp success rate for novel objects presented in dense clutter.},
archivePrefix = {arXiv},
arxivId = {1706.09911},
author = {ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
doi = {10.1177/0278364917735594},
eprint = {1706.09911},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/tenpas2017.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {grasp detection,grasping,manipulation,perception,tenPas2017},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
mendeley-tags = {tenPas2017},
month = {dec},
number = {13-14},
pages = {1455--1473},
title = {{Grasp Pose Detection in Point Clouds}},
url = {http://journals.sagepub.com/doi/10.1177/0278364917735594},
volume = {36},
year = {2017}
}


@article{Chu2018,
abstract = {A deep learning architecture is proposed to predict graspable locations for robotic manipulation. It considers situations where no, one, or multiple object(s) are seen. By defining the learning problem to be classified with null hypothesis competition instead of regression, the deep neural network with red, green, blue and depth (RGB-D) image input predicts multiple grasp candidates for a single object or multiple objects, in a single shot. The method outperforms state-of-the-art approaches on the Cornell dataset with 96.0{\%} and 96.1{\%} accuracy on imagewise and objectwise splits, respectively. Evaluation on a multiobject dataset illustrates the generalization capability of the architecture. Grasping experiments achieve 96.0{\%} grasp localization and 89.0{\%} grasping success rates on a test set of household objects. The real-time process takes less than 0.25 s from image to plan.},
annote = {Este paper tem uma revisao bibliografica historica interessante de learning approaches},
archivePrefix = {arXiv},
arxivId = {1802.00520},
author = {Chu, Fu-Jen and Xu, Ruinian and Vela, Patricio A.},
doi = {10.1109/LRA.2018.2852777},
eprint = {1802.00520},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/1802.00520.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Perception for grasping,deep learning in robotic automation,grasping},
mendeley-groups = {Grasping/Data-driven,Grasping/Data-driven/Deep Learning},
month = {oct},
number = {4},
pages = {3355--3362},
title = {{Real-World Multiobject, Multigrasp Detection}},
url = {https://ieeexplore.ieee.org/document/8403246/},
volume = {3},
year = {2018}
}

@inproceedings{asif2018ensemblenet,
  title={EnsembleNet: Improving Grasp Detection using an Ensemble of Convolutional Neural Networks.},
  author={Asif, Umar and Tang, Jianbin and Harrer, Stefan},
  booktitle={BMVC},
  pages={10},
  year={2018}
}

@article{Chen2019,
abstract = {Generally, most grasp detection models follow the similar frameworks as that in object detection, which use the convolutional neural network to regress the grasp parameters directly. However, grasp detection and object detection are actually different, for the ground truths in object detection are unique while that in grasp detection are not exhaustive. A predicted grasp could still be applicable despite it does not coincide well with ground truth. In this paper, a novel grasp detection model is constructed to make a fairer evaluation on grasp candidate. Instead of using isolated ground truths, the grasp path is introduced to reveal the possible consequent distribution of ground truths. The grasp candidate is first mapped to grasp path, generating the mapped grasp, and the bias between them works as the estimated error for back-propagation. Experiments deployed on grasping dataset as well as real-world scenarios show that our proposed method could improve the detection accuracy. In addition, it can be well-generalized to detect unseen objects.},
author = {Chen, Lu and Huang, Panfeng and Meng, Zhongjie},
doi = {10.1016/j.robot.2019.01.009},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/chen2019.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Convolutional neural network,Grasp path,Multi-grasp detection,lu2019},
mendeley-groups = {Grasping/Data-driven,Grasping/Data-driven/Deep Learning},
mendeley-tags = {lu2019},
month = {mar},
pages = {94--103},
publisher = {Elsevier B.V.},
title = {{Convolutional multi-grasp detection using grasp path for RGBD images}},
url = {https://doi.org/10.1016/j.robot.2019.01.009 https://linkinghub.elsevier.com/retrieve/pii/S0921889018307346},
volume = {113},
year = {2019}
}



@article{Ghazaei2019,
abstract = {Humans excel in grasping and manipulating objects because of their life-long experience and knowledge about the 3D shape and weight distribution of objects. However, the lack of such intuition in robots makes robotic grasping an exceptionally challenging task. There are often several equally viable options of grasping an object. However, this ambiguity is not modeled in conventional systems that estimate a single, optimal grasp position. We propose to tackle this problem by simultaneously estimating multiple grasp poses from a single RGB image of the target object. Further, we reformulate the problem of robotic grasping by replacing conventional grasp rectangles with grasp belief maps, which hold more precise location information than a rectangle and account for the uncertainty inherent to the task. We augment a fully convolutional neural network with a multiple hypothesis prediction model that predicts a set of grasp hypotheses in under 60{\^{A}} ms, which is critical for real-time robotic applications. The grasp detection accuracy reaches over {\$}{\$}90$\backslash${\%}{\$}{\$} for unseen objects, outperforming the current state of the art on this task.},
archivePrefix = {arXiv},
arxivId = {1811.00793},
author = {Ghazaei, Ghazal and Laina, Iro and Rupprecht, Christian and Tombari, Federico and Navab, Nassir and Nazarpour, Kianoush},
doi = {10.1007/978-3-030-20870-7_3},
eprint = {1811.00793},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/1811.00793.pdf:pdf},
isbn = {9783030208691},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep learning,Multiple hypotheses,Robotic grasping,ghazaei2018},
mendeley-tags = {ghazaei2018},
pages = {38--55},
title = {{Dealing with Ambiguity in Robotic Grasping via Multiple Predictions}},
volume = {11364 LNCS},
year = {2019}
}

@InProceedings{Mousavian_2019_ICCV,
author = {Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
title = {6-DOF GraspNet: Variational Grasp Generation for Object Manipulation},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
} 

@article{Chen2020,
abstract = {Most convolutional neural network based grasp detection methods evaluate the predicted grasp by computing its overlap with the selected ground truth grasp. But for typical grasp datasets, not all graspable examples are labelled as ground truths. Hence, directly back propagating the generated loss during training could not fully reveal the graspable ability of the predicted grasp. In this paper, we integrate the grasp mapping mechanism with the convolutional neural network, and propose a multi-scale, multi-grasp detection model. First, we connect each labeled grasp and refine them by discarding inconsistent and redundant connections to form the grasp path. Then, the predicted grasp is mapped to the grasp path and the error between them is used for back-propagation as well as grasp evaluation. Last, they are combined into the multi-grasp detection framework to detect grasps with efficiency. Experimental results both on Cornell Grasping Dataset and real-world robotic grasping system verify the effectiveness of our proposed method. In addition, its detection accuracy keeps relatively stable even in the circumstance of high Jaccard threshold.},
author = {Chen, Lu and Huang, Panfeng and Li, Yuanhao and Meng, Zhongjie},
doi = {10.1007/s12555-019-0186-2},
file = {:C$\backslash$:/Users/Avell B155 MAX/Downloads/10.1007@s12555-019-0186-2.pdf:pdf},
isbn = {1255501901},
issn = {1598-6446},
journal = {International Journal of Control, Automation and Systems},
keywords = {Convolutional neural network,grasp detection,grasp path,lu2020,robotic grasping},
mendeley-groups = {Grasping/Data-driven},
mendeley-tags = {lu2020},
month = {feb},
number = {X},
pages = {1--10},
title = {{Detecting Graspable Rectangles of Objects in Robotic Grasping}},
url = {http://link.springer.com/10.1007/s12555-019-0186-2},
volume = {18},
year = {2020}
}


@article{Gariepy2019,
abstract = {Grasping is a fundamental robotic task needed for the deployment of household robots or furthering warehouse automation. However, few approaches are able to perform grasp detection in real time (frame rate). To this effect, we present Grasp Quality Spatial Transformer Network (GQ-STN), a one-shot grasp detection network. Being based on the Spatial Transformer Network (STN), it produces not only a grasp configuration, but also directly outputs a depth image centered at this configuration. By connecting our architecture to an externally-trained grasp robustness evaluation network, we can train efficiently to satisfy a robustness metric via the backpropagation of the gradient emanating from the evaluation network. This removes the difficulty of training detection networks on sparsely annotated databases, a common issue in grasping. We further propose to use this robustness classifier to compare approaches, being more reliable than the traditional rectangle metric. Our GQ-STN is able to detect robust grasps on the depth images of the Dex-Net 2.0 dataset with 92.4 {\%} accuracy in a single pass of the network. We finally demonstrate in a physical benchmark that our method can propose robust grasps more often than previous sampling-based methods, while being more than 60 times faster.},
annote = {This papers is good in one aspect, they try to explore the fact that rectangle approach is not a good approach. They proved it with experimental results. 

I just dont understand, they usethe GQ-CNN to evaluate their work that is, also, based in CQ-CNN. I dont understand its perfomance and improvement. That is why I dont like the grasping prediction made by them.},
archivePrefix = {arXiv},
arxivId = {1903.02489},
author = {Gariepy, Alexandre and Ruel, Jean Christophe and Chaib-Draa, Brahim and Giguere, Philippe},
doi = {10.1109/IROS40897.2019.8967785},
eprint = {1903.02489},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gariepy et al. - 2019 - GQ-STN Optimizing One-Shot Grasp Detection based on Robustness Classifier.pdf:pdf},
isbn = {9781728140049},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {gariepy2019},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
mendeley-tags = {gariepy2019},
pages = {3996--4003},
title = {{GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness Classifier}},
year = {2019}
}

@article{song2020novel,
  title={A novel robotic grasp detection method based on region proposal networks},
  author={Song, Yanan and Gao, Liang and Li, Xinyu and Shen, Weiming},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={65},
  pages={101963},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{Pinto2015,
abstract = {— Current learning-based robot grasping ap-proaches exploit human-labeled datasets for training the mod-els. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
author = {Pinto, Lerrel and Gupta, Abhinav},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487517},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinto, Gupta - 2015 - href{\{}httpsarxiv.orgabs1509.06825{\}}{\{}Supersizing Self-supervision Learning to Grasp from 50K Tries and 700 Robot Hour.pdf:pdf}},
isbn = {978-1-4673-8026-3},
mendeley-groups = {Bin Picking/IA Machine Learning and Deep Learning,Grasping,Bin Picking/Pose Estimation,Bin Picking/Recognition},
month = {may},
pages = {3406--3413},
publisher = {IEEE},
title = {{Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours}},
url = {http://ieeexplore.ieee.org/document/7487517/},
year = {2016}
}


@article{Levine2018,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02199v4},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
doi = {10.1177/0278364917710318},
eprint = {arXiv:1603.02199v4},
file = {:C$\backslash$:/Users/JoaoPedro/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine et al. - 2018 - Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Robotics,deep learning,neural networks},
mendeley-groups = {Grasping,Bin Picking/Pose Estimation,To READ/Reinforcement Learning},
month = {apr},
number = {4-5},
pages = {421--436},
title = {{Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection}},
url = {http://journals.sagepub.com/doi/10.1177/0278364917710318},
volume = {37},
year = {2018}
}




%% DexNet Works from Berkely University

@inproceedings{Mahler2017b,
abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93{\%} on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99{\%} precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
arxivId = {1703.09312},
author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Aparicio, Juan and Goldberg, Ken},
booktitle = {Robotics: Science and Systems XIII},
doi = {10.15607/RSS.2017.XIII.058},
eprint = {1703.09312},
file = {:home/joaopedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics(2).pdf:pdf},
isbn = {978-0-9923747-3-0},
keywords = {berkeley2},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
mendeley-tags = {berkeley2},
month = {jul},
publisher = {Robotics: Science and Systems Foundation},
title = {{Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics}},
url = {http://arxiv.org/abs/1703.09312 http://www.roboticsproceedings.org/rss13/p58.pdf},
year = {2017}
}

@inproceedings{alonso2018current,
  title={Current research trends in robot grasping and bin picking},
  author={Alonso, Marcos and Izaguirre, Alberto and Gra{\~n}a, Manuel},
  booktitle={The 13th International Conference on Soft Computing Models in Industrial and Environmental Applications},
  pages={367--376},
  year={2018},
  organization={Springer}
}


@article{Mahler2019,
abstract = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores “ambidextrous” robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95{\%} at a rate of more than 300 mean picks per hour.},
author = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Ken},
doi = {10.1126/scirobotics.aau4984},
file = {:home/joaopedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahler et al. - 2019 - Learning ambidextrous robot grasping policies.pdf:pdf},
journal = {Science Robotics},
keywords = {berkeley5},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
mendeley-tags = {berkeley5},
number = {26},
pages = {eaau4984},
title = {{Learning ambidextrous robot grasping policies}},
volume = {4},
year = {2019}
}

@article{Mahler2017d,
abstract = {Recent results suggest that it is possible to grasp a variety of singu-lated objects with high precision using Convolutional Neural Networks (CNNs) trained on synthetic data. This paper considers the task of bin picking, where multiple objects are randomly arranged in a heap and the objective is to sequentially grasp and transport each into a packing box. We model bin picking with a discrete-time Partially Observable Markov Decision Process that specifies states of the heap, point cloud observations, and rewards. We collect synthetic demonstrations of bin picking from an algorithmic supervisor uses full state information to optimize for the most robust collision-free grasp in a forward simulator based on pybullet to model dynamic object-object interactions and robust wrench space analysis from the Dexterity Network (Dex-Net) to model quasi-static contact between the gripper and object. We learn a policy by fine-tuning a Grasp Quality CNN on Dex-Net 2.1 to classify the supervisor's actions from a dataset of 10,000 rollouts of the supervisor in the simulator with noise injection. In 2,192 physical trials of bin picking with an ABB YuMi on a dataset of 50 novel objects, we find that the resulting policies can achieve 94{\%} success rate and 96{\%} average precision (very few false positives) on heaps of 5-10 objects and can clear heaps of 10 objects in under three minutes. Datasets, experiments, and supplemental material are available at http://berkeleyautomation.github.io/dex-net.},
author = {Mahler, Jeffrey and Goldberg, Ken},
file = {:home/joaopedro/Downloads/mahler17a.pdf:pdf},
journal = {Conference on robot learning},
keywords = {Grasping,Imitation Learning,Simulation},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
number = {CoRL},
pages = {515--524},
title = {{Learning deep policies for robot bin picking by simulating robust grasping sequences}},
url = {http://berkeleyautomation.github.io/dex-net.},
year = {2017}
}

@article{Mahler2017,
abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98{\$}\backslash{\%}{\$}, 82{\$}\backslash{\%}{\$}, and 58{\$}\backslash{\%}{\$} respectively, improving to 81{\$}\backslash{\%}{\$} in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
archivePrefix = {arXiv},
arxivId = {1709.06670},
author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
eprint = {1709.06670},
file = {:home/joaopedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahler et al. - 2017 - Dex-Net 3.0 Computing Robust Robot Vacuum Suction Grasp Targets in Point Clouds using a New Analytic Model and De.pdf:pdf},
mendeley-groups = {Bin Picking,Grasping/Data-driven/Deep Learning},
title = {{Dex-Net 3.0: Computing Robust Robot Vacuum Suction Grasp Targets in Point Clouds using a New Analytic Model and Deep Learning}},
url = {http://arxiv.org/abs/1709.06670},
year = {2017}
}

@inproceedings{Mahler2016,
abstract = {This paper presents the Dexterity Network (Dex-Net) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a Multi- Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/.},
author = {Mahler, Jeffrey and Pokorny, Florian T. and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kroger, Torsten and Kuffner, James and Goldberg, Ken},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487342},
file = {:home/joaopedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahler et al. - 2016 - Dex-Net 1.0 A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with c.pdf:pdf},
isbn = {978-1-4673-8026-3},
issn = {10504729},
mendeley-groups = {Grasping/Data-driven/Deep Learning},
month = {may},
pages = {1957--1964},
publisher = {IEEE},
title = {{Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards}},
url = {http://ieeexplore.ieee.org/document/7487342/},
volume = {2016-June},
year = {2016}
}

@article{jaskowski2018improved,
  title={Improved GQ-CNN: Deep learning model for planning robust grasps},
  author={Ja{\'s}kowski, Maciej and {\'S}wi{\k{a}}tkowski, Jakub and Zaj{\k{a}}c, Micha{\l} and Klimek, Maciej and Potiuk, Jarek and Rybicki, Piotr and Polatowski, Piotr and Walczyk, Przemys{\l}aw and Nowicki, Kacper and Cygan, Marek},
  journal={arXiv preprint arXiv:1802.05992},
  year={2018}
}


% Graspit Related

@article{miller2004graspit,
  title={Graspit! a versatile simulator for robotic grasping},
  author={Miller, Andrew T and Allen, Peter K},
  journal={IEEE Robotics \& Automation Magazine},
  volume={11},
  number={4},
  pages={110--122},
  year={2004},
  publisher={IEEE}
}

@article{Ciocarlie2009,
abstract = {In this paper we focus on the concept of low-dimensional posture subspaces for artificial hands. We begin by discussing the applicability of a hand configuration subspace to the problem of automated grasp synthesis; our results show that low-dimensional optimization can be instrumental in deriving effective pre-grasp shapes for a number of complex robotic hands. We then show that the computational advantages of using a reduced dimensionality framework enable it to serve as an interface between the human and automated components of an interactive grasping system. We present an on-line grasp planner that allows a human operator to perform dexterous grasping tasks using an artificial hand. In order to achieve the computational rates required for effective user interaction, grasp planning is performed in a hand posture subspace of highly reduced dimensionality. The system also uses real-time input provided by the operator, further simplifying the search for stable grasps to the point where solutions can be found at interactive rates. We demonstrate our approach on a number of different hand models and target objects, in both real and virtual environments.},
annote = {Otimo paper descrevendo o processo e otimiza{\c{c}}{\~{a}}o por SANN do graspit.},
author = {Ciocarlie, Matei T. and Allen, Peter K.},
doi = {10.1177/0278364909105606},
file = {:C$\backslash$:/Users/JoaoPedro/OneDrive - INESC TEC/Bin-Picking/Artigos/Grasp/eigengrasps.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dexterous robotic hands,Hand prosthetics,Interactive grasping},
number = {7},
pages = {851--867},
title = {{Hand posture subspaces for dexterous robotic grasping}},
volume = {28},
year = {2009}
}

@article{Santello2002,
annote = {Paper que foi usado como base para o eigengrasp},
author = {Santello, Marco and Flanders, Martha and Soechting, John F},
file = {:C$\backslash$:/Users/JoaoPedro/Desktop/artigos nvoos/neuro{\_}grasp.pdf:pdf},
keywords = {alternatively,fingers,grasping,guided,is there a proximal-to-distal,kinematics,memory,motion,or is the pattern,progression of finger,synergies,to address these questions,variable,visually guided,we},
number = {4},
pages = {1426--1435},
title = {{Patterns of Hand Motion during Grasping and the Influence of Sensory Guidance}},
volume = {22},
year = {2002}
}

@article{kirkpatrick1983,
annote = {Este e o paper que introduziu p simulated annealing},
author = {Kirkpatrick, S and Gelatt, C D and Vecchi, M P},
file = {:C$\backslash$:/Users/JoaoPedro/Desktop/artigos nvoos/primeiropaper{\_}sann.pdf:pdf},
journal = {Science},
number = {4598},
pages = {671--680},
title = {{Optimization by Simulated Annealing}},
volume = {220},
year = {1983}
}

@article{ingber1988,
annote = {Paper introduzindo o very fast Simulated Annealing},
author = {L. Ingber},
file = {:C$\backslash$:/Users/JoaoPedro/Desktop/artigos nvoos/fast{\_}snn.pdf:pdf},
number = {8},
pages = {967--973},
title = {{Very Fast Simulated Re-Annealing}},
volume = {12},
year = {1989}
}

%>Deep RL
@article{james2017transferring,
  title={Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task},
  author={James, Stephen and Davison, Andrew J and Johns, Edward},
  journal={arXiv preprint arXiv:1707.02267},
  year={2017}
}

@article{viereck2017learning,
  title={Learning a visuomotor controller for real world robotic grasping using simulated depth images},
  author={Viereck, Ulrich and Pas, Andreas ten and Saenko, Kate and Platt, Robert},
  journal={arXiv preprint arXiv:1706.04652},
  year={2017}
}



%>Extras
@inproceedings{su2015multi,
  title={Multi-view convolutional neural networks for 3d shape recognition},
  author={Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={945--953},
  year={2015}
}

@article{goodale1991neurological,
  title={A neurological dissociation between perceiving objects and grasping them},
  author={Goodale, Melvyn A and Milner, A David and Jakobson, LS and Carey, DP},
  journal={Nature},
  volume={349},
  number={6305},
  pages={154--156},
  year={1991},
  publisher={Nature Publishing Group}
}

@article{castiello2005neuroscience,
  title={The neuroscience of grasping},
  author={Castiello, Umberto},
  journal={Nature Reviews Neuroscience},
  volume={6},
  number={9},
  pages={726--736},
  year={2005},
  publisher={Nature Publishing Group}
}

@book{murray1994mathematical,
  title={A mathematical introduction to robotic manipulation},
  author={Murray, Richard M and Li, Zexiang and Sastry, S Shankar and Sastry, S Shankara},
  year={1994},
  publisher={CRC press}
}

% Arquiteturas de CNN
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}



% Usou o Graspit
@inproceedings{morales2006integrated,
  title={Integrated grasp planning and visual object localization for a humanoid robot with five-fingered hands},
  author={Morales, Antonio and Asfour, Tamim and Azad, Pedram and Knoop, Steffen and Dillmann, Rudiger},
  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5663--5668},
  year={2006},
  organization={IEEE}
}

%>Databases
@misc{conerll_database,
      title={Cornell Dataset},
      author={Robot Learning Lab}, 
      url={http://pr.cs.cornell.edu/grasping/rect\_data/data.php},
      journal={Personal Robotics: Grasping},
      note = {(Accessed 2020-05-08)},
      }
      
@misc{jacquard_dataset,
      title={Jacquard dataset},
      url={https://jacquard.liris.cnrs.fr/},
      year={2018},
      journal={JACQUARD DATASET},
      note = {(Accessed 2020-05-08)},
      }
      
%>Challenges
@misc{amazon_challenge,
      title={The Future of Robotics},
      url={http://amazonpickingchallenge.org/},
      journal={Amazon Picking Challenge},
      author={Lawrence, Charlotte},
      note = {(Accessed 2020-05-19)},
      }
      
@misc{imagenet_challenge,
      title={Large Scale Visual Recognition Challenge (ILSVRC)},
      url={http://www.image-net.org/challenges/LSVRC/},
      journal={ImageNet Large Scale Visual Recognition Competition (ILSVRC)},
      note = {(Accessed 2020-05-19)},
      }
      
@misc{cuda,
    title={CUDA Toolkit Documentation v11.0.3},
    url={https://docs.nvidia.com/cuda/index.html},
    journal={CUDA Toolkit Documentation},
    year=2022,
    publisher={Nvidea},
} 

@misc{iros_challenge,
      title={Robotic Grasping and Manipulation Competition},
      url={https://rpal.cse.usf.edu/competition_iros2020/},
      journal={Amazon Picking Challenge},
      author={IROS},
      year={2020},
      note = {(Accessed 2022-05-02)},
      }
      
@misc{urdf,
      title={URDF},
      url={http://wiki.ros.org/urdf},
      journal={ROS},
      author={ROS},
      note = {(Accessed 2022-05-05)},
      }
      
      




@inproceedings{maturana2015voxnet,
  title={Voxnet: A 3d convolutional neural network for real-time object recognition},
  author={Maturana, Daniel and Scherer, Sebastian},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={922--928},
  year={2015},
  organization={IEEE}
}

% To RCIM >< remove if it will not pub in RCIM

@article{roa2009finding,
  title={Finding locally optimum force-closure grasps},
  author={Roa, M{\'a}ximo A and Su{\'a}rez, Ra{\'u}l},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={25},
  number={3},
  pages={536--544},
  year={2009},
  publisher={Elsevier}
}

@article{suleman2011learning,
  title={Learning from demonstration in robots: Experimental comparison of neural architectures},
  author={Suleman, Muhammad Umar and Awais, Mian M},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={27},
  number={4},
  pages={794--801},
  year={2011},
  publisher={Elsevier}
}

@article{d2020study,
  title={A study on picking objects in cluttered environments: Exploiting depth features for a custom low-cost universal jamming gripper},
  author={D’Avella, Salvatore and Tripicchio, Paolo and Avizzano, Carlo Alberto},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={63},
  pages={101888},
  year={2020},
  publisher={Elsevier}
}

@article{chen2020active,
  title={Active curved surface deforming of flexible conformal electronics by multi-fingered actuator},
  author={Chen, Jiankui and Yang, Sihui and Li, Yiqun and Huang, Yongan and Yin, Zhouping},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={64},
  pages={101942},
  year={2020},
  publisher={Elsevier}
}

@article{babin2019stable,
  title={Stable and repeatable grasping of flat objects on hard surfaces using passive and epicyclic mechanisms},
  author={Babin, Vincent and St-Onge, David and Gosselin, Cl{\'e}ment},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={55},
  pages={1--10},
  year={2019},
  publisher={Elsevier}
}

@article{delgado2017hand,
  title={In-hand recognition and manipulation of elastic objects using a servo-tactile control strategy},
  author={Delgado, A and Jara, CA and Torres, F},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={48},
  pages={102--112},
  year={2017},
  publisher={Elsevier}
}

@article{bjornsson2018automated,
  title={Automated material handling in composite manufacturing using pick-and-place systems--a review},
  author={Bj{\"o}rnsson, Andreas and Jonsson, Marie and Johansen, Kerstin},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={51},
  pages={222--229},
  year={2018},
  publisher={Elsevier}
}

@article{birglen2018statistical,
  title={A statistical review of industrial robotic grippers},
  author={Birglen, Lionel and Schlicht, Thomas},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={49},
  pages={88--97},
  year={2018},
  publisher={Elsevier}
}

@ARTICLE{moreira_2016,
    author={E. {Moreira} and L. F. {Rocha} and A. M. {Pinto} and A. P. {Moreira} and G. {Veiga}},  journal={IEEE Robotics and Automation Letters}, 
    title={Assessment of Robotic Picking Operations Using a 6 Axis Force/Torque Sensor},   year={2016},
    volume={1},
    number={2},
    pages={768-775},
    doi={10.1109/LRA.2016.2524043}}
    
@article{ferreira2016stereo,
  title={Stereo-based real-time 6-DoF work tool tracking for robot programing by demonstration},
  author={Ferreira, Marcos and Costa, Paulo and Rocha, Lu{\'\i}s and Moreira, A Paulo},
  journal={The International Journal of Advanced Manufacturing Technology},
  volume={85},
  number={1},
  pages={57--69},
  year={2016},
  publisher={Springer}
}

@article{COSTA2016113,
title = {Robust 3/6 DoF self-localization system with selective map update for mobile robot platforms},
journal = {Robotics and Autonomous Systems},
volume = {76},
pages = {113-140},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2015.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015002250},
author = {Carlos M. Costa and Héber M. Sobreira and Armando J. Sousa and Germano M. Veiga},
keywords = {Self-localization, Simultaneous localization and mapping, Point cloud registration, Geometric feature matching},
abstract = {Mobile robot platforms capable of operating safely and accurately in dynamic environments can have a multitude of applications, ranging from simple delivery tasks to advanced assembly operations. These abilities rely heavily on a robust navigation stack, which requires stable and accurate pose estimations within the environment. To solve this problem, a modular localization system suitable for a wide range of mobile robot platforms was developed. By using LIDAR/RGB-D data, the proposed system is capable of achieving 1–2 cm in translation error and 1°–3° degrees in rotation error while requiring only 5–35 ms of processing time (in 3 and 6 DoF respectively). The system was tested in three robot platforms and in several environments with different sensor configurations. It demonstrated high accuracy while performing pose tracking with point cloud registration algorithms and high reliability when estimating the initial pose using feature matching techniques. The system can also build a map of the environment with surface reconstruction and incrementally update it with either the full field of view of the sensor data or only the unknown sections, which allows to reduce the mapping processing time and also gives the possibility to update a CAD model of the environment without degrading the detail of known static areas due to sensor noise.}
}


@misc{pr2robot,
      title={PR2 - ROBOTS},
      url={https://robots.ieee.org/robots/pr2/},
       year={2010},
      journal={Robots},
      author={IEEE Spectrum},
      note = {(Accessed 2022-06-22)},
      }
	  
@misc{tracik,
  title={track\_ik},
  url={https://bitbucket.org/traclabs/trac_ik/src/master/},
  journal={},
  author={TracLabs},
  note = {(Accessed 2022-07-22)},
  year = 2021,
  }  
  
	  


% Livros
@book{monkman2007robot,
  title={Robot grippers},
  author={Monkman, Gareth J and Hesse, Stefan and Steinmann, Ralf and Schunk, Henrik},
  year={2007},
  publisher={John Wiley \& Sons}
}

% commercial grippers





@misc{barret_hand,
      title={Barrett Hand},
      url={https://www.roscomponents.com/en/robotic-hands/16-barrett-bh8-282.html#/bhand_rbk_strain_br_barrett_hand-no/bhand_rbk_ftsensor_barrett_hand-no/b_hand_rbk_tactile_br_barrett_hand-no/b_hand_rbk_palm_br_barrett_hand-no/b_hand_rbk_controll_br_barrett_hand-no/b_hand_rbk_wam_br_barrett_hand-no/b_hand_rbk_gimbal_br_barrett_hand-no},
      journal={},
      author={ROS Components},
      year={2016},
      note = {(Accessed 2022-08-19)},
      }



@misc{magnetic,
      title={Magnetic grippers},
      url={https://www.goudsmitmagnets.com/solutions/magnetic-handling/lifting-handling-magnets/magnetic-grippers},
      journal={},
      author={Goudsmit Magnetics},
      year={2022},
      note = {(Accessed 2022-05-06)},
      }
      
      
@misc{parallel,
      title={PGN-plus-E},
      url={https://schunk.com/us_en/gripping-systems/series/pgn-plus-e/},
      journal={},
      year={2022},
      author={Schunk},
      note = {(Accessed 2022-05-06)},
      }
      
@misc{angular,
      title={PWG-plus},
      url={https://schunk.com/de_en/gripping-systems/series/pwg-plus/},
      journal={},
      author={Schunk},
      note = {(Accessed 2022-05-06)},
       year={2022},
      }
      
@misc{robotiq_grippers,
      title={2F-85 and 2F-140 Grippers},
      url={https://robotiq.com/products/2f85-140-adaptive-robot-gripper},
      journal={},
      author={Robotiq},
      note = {(Accessed 2022-05-06)},
      year={2022},
      }

@misc{bernoulli,
      title={Bernoulli grippers OGGB},
      url={https://www.festo.com/lt/en/p/bernoulli-grippers-id_OGGB/},
      journal={},
      author={Festo},
      note = {(Accessed 2022-05-06)}, 
      year={2022},
      }
     
@misc{dual_suction,
      title={piCOBOT},
      url={https://www.piab.com/robot-and-cobot-gripping-solutions/cobots-and-robot-grippers/picobot-vacuum-gripper-unit/picobot/},
      journal={},
      author={piab},
      note = {(Accessed 2022-05-06)},
      year={2022},
      }
      
      
@misc{anthopomorphic,
      title={VH Hand},
      url={https://www.roscomponents.com/en/robotic-hands/114-svh-hand.html#/svh_hand_training-no/svh_hand_left_or_right_hand-left_hand},
      journal={},
      author={ROS Components},
      note = {(Accessed 2022-05-06)},
       year={2022},
      }   
      
@misc{versaball,
      title={Versaball},
      url={https://www.empirerobotics.com/products/},
      journal={},
      author={Empire Robotics},
      note = {(Accessed 2022-05-06)},
       year={2022},
      }

@misc{multi_gripper_nsr,
      title={NSR-MTM-3-URe Multi-Tool Mount System for e-Series Universal Robots (UR)},
      url={https://www.newscalerobotics.com/products/nsr-mtm/},
      journal={},
      author={New Scale Robotics},
      note = {(Accessed 2022-05-06)},
      year={2022},
      }
      
@misc{abb_dual_arm,
      title={Dual-arm YuMI IRB 14000},
      url={https://webshop.robotics.abb.com/us/catalog/product/view/id/84/s/dual-arm-yumi-irb-14000-assembly/category/3/},
      journal={},
      author={ABB},
      year={2022},
      note = {(Accessed 2022-05-06)},
      }
      
%softwares 
@misc{or,
      title={Object Recognition},
      url={https://github.com/carlosmccosta/object_recognition},
      journal={},
      author={Carlos M Costa},
      year={2022},
      note = {(Accessed 2022-05-24)},
      }
      
  
%tools          
@misc{ros_action_lib,
      title={ROS Action Library},
      url={http://wiki.ros.org/actionlib},
      journal={},
      author={ROS},
      note = {(Accessed 2022-05-10)},
	  year = {2018}, 
      }
      
@misc{rviz,
      title={RViz},
      url={http://wiki.ros.org/rviz},
      journal={},
      author={ROS},
      note = {(Accessed 2022-05-13)},
	  year = 2018,
      }
      
@misc{schmalz_cup,
      title={ FM-SW 76x22 4x6 N10},
      url={https://www.schmalz.com/en/vacuum-technology-for-automation/vacuum-components/area-gripping-systems-and-end-effectors/vacuum-area-gripping-systems-fx-fm/area-gripping-systems-fm-sw-306896/10.01.11.00851/},
      journal={},
      author={Schmalz},
      year          =  2022,
      note = {(Accessed 2022-05-20)},
      }
      
@misc{photoneo,
      title={Phoxi 3D Scanner S},
      url={https://www.photoneo.com/products/phoxi-scan-s/},
      journal={},
      author={Photoneo},
      year          =  2022,
      note = {(Accessed 2022-05-24)},
      }
      
@misc{festo_2f,
      title={Parallel gripper HGPC-16-A},
      url={},
      journal={},
      author={FESTO},
      year =  2017,
      note = {(Accessed 2022-08-09)},
      }
      

 @misc{impressora3d,
    organization  = "Raise3D",
    title         = "Pro2 Plus 3D Printer",
    url={https://www.raise3d.com/products/pro2-plus-3d-printer/},
    number        = "",
    year          = 2022,
    month         = 11, 
    note = {(Accessed 2022-08-01)},
}





%my pubs

@article{DESOUZA2021102176,
title = {Robotic grasping: from wrench space heuristics to deep learning policies},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {71},
pages = {102176},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102176},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000594},
author = {João Pedro Carvalho {de Souza} and Luís F. Rocha and Paulo Moura Oliveira and A. Paulo Moreira and José Boaventura-Cunha},
keywords = {Industrial robotic grasp, Grasping detection, Grasping planning, Grasping selection},
abstract = {The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.}
}

@article{carvalho2020,
title = "Reconfigurable Grasp Planning Pipeline with Grasp Synthesis and Selection Applied to Picking Operations in Aerospace Factories",
journal = "Robotics and Computer-Integrated Manufacturing",
volume = "67",
pages = "102032",
year = "2021",
issn = "0736-5845",
doi = "https://doi.org/10.1016/j.rcim.2020.102032",
url = "http://www.sciencedirect.com/science/article/pii/S073658452030243X",
author = "de Souza, João Pedro C. and Costa, Carlos and Rocha, Luís and Arrais, Rafael  and Moreira, A.Paulo and Pires, E.J. and Boaventura, José",
keywords = "Grasping Pipeline, Robot Pick and Place, Manufacturing Robot Processes",
abstract = "Several approaches with interesting results have been proposed over the years for robot grasp planning. However, the industry suffers from the lack of an intuitive and reliable system able to automatically estimate grasp poses while also allowing the integration of grasp information from the accumulated knowledge of the end user. In the presented paper it is proposed a non-object-agnostic grasping pipeline motivated by picking use cases from the aerospace industry. The planning system extends the functionality of the simulated annealing optimization algorithm for allowing its application within an industrial use case. Therefore, this paper addresses the first step of the design of a reconfigurable and modular grasping pipeline. The key idea is the creation of an intuitive and functional grasping framework for being used by factory floor operators according to the task demands. This software pipeline is capable of generating grasp solutions in an offline phase, and later on, in the robot operation phase, can choose the best grasp pose by taking into consideration a set of heuristics that try to achieve a successful grasp while also requiring the least effort for the robotic arm. The results are presented in a simulated and a real factory environment, relying on a mobile platform developed for intralogistic tasks. With this architecture, new state-of-art methodologies can be integrated in the future for growing the grasping pipeline and make it more robust and applicable to a wider range of use cases."
}

@article{6dmimicIMU,
title = {Industrial Robot Programming by Demonstration using Stereoscopic Vision and Inertial Sensing},
journal={Industrial Robot: the international journal of robotics research and application},
publisher={Emerald Publishing Limited},
volume = {},
pages = {},
year = {2021},
issn = {0143-991x},
doi = {10.1108/IR-02-2021-0043},
url = {},
author = {João Pedro Carvalho {de Souza} and Ant\'{o}nio Amorim and V\'{i}tor H. Pinto and Lu\'{i}s F. Rocha and Ant\'{o}nio P. Moreira},
keywords = {Industrial robotic grasp, Grasping detection, Grasping planning, Grasping selection},
abstract = {}
}


% Projetos

@misc{mari4yard,
    organization  = "Mari4Alliance",
    title         = "Mari4Yard",
    url= {https://www.mari4yard.eu/#About},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-01)},
}

@misc{produtech4sc,
    organization  = "PROGRAMAS MOBILIZADORES PRODUTECH",
    title         = "Produtech 4S\&C.",
    url= {http://mobilizadores.produtech.org/pt/produtech_4_s_c},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-01)},
}

@misc{fasten,
    organization  = "INESCTEC - CRIIS",
    title         = "Fasten",
    url= {https://criis.inesctec.pt/index.php/criis-projects/fasten/},
    number        = "",
    year          = 2020,
    month         = 08, 
    note = {(Accessed 2022-08-01)},
}

@misc{criis,
    organization  = "INESCTEC - CRIIS",
    title         = "Centro de Robótica Industrial e Sistemas Inteligentes",
    url= {https://www.inesctec.pt/pt/centros/criis},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-01)},
}

@misc{inesctec,
    organization  = "INESCTEC",
    title         = "INESCTEC",
    url= {https://www.inesctec.pt/pt},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-01)},
}

%Empresas



@misc{embraer_pt,
    organization  = "Embraer",
    title         = "Embraer Portugal",
    url= {https://embraerportugal.gupy.io/},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-09)},
}


@misc{jpm,
    organization  = "JPM",
    title         = "JPM",
    url= {https://jpm.pt/pt/inicio//},
    number        = "",
    year          = 2022,
    month         = 08, 
    note = {(Accessed 2022-08-09)},
}

%referenciaram meu trabalho
@article{hu2022grasps,
  title={A grasps-generation-and-selection convolutional neural network for a digital twin of intelligent robotic grasping},
  author={Hu, Weifei and Wang, Chuxuan and Liu, Feixiang and Peng, Xiang and Sun, Pengwen and Tan, Jianrong},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={77},
  pages={102371},
  year={2022},
  publisher={Elsevier}
}





@Comment{jabref-meta: databaseType:bibtex;}
